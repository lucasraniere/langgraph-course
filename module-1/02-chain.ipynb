{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d1e9cb",
   "metadata": {},
   "source": [
    "### **Chain**\n",
    "\n",
    "Building up a simple **chain** that combines 4 key concepts:\n",
    "* Using chat messages in our graph\n",
    "* Using chat models\n",
    "* Biding tools to our LLM\n",
    "* Executing tool calls in our graph\n",
    "\n",
    "##### **Messages**\n",
    "\n",
    "Chat models can use `messages`, which capture different roles within a conversation.\n",
    "\n",
    "LangChain supports various messages types, including `HumanMessage`, `AIMessage`, `SystemMessage`, and `ToolMessage`.\n",
    "\n",
    "These represent a message from user, chat model, for the chat model to instruct behavior, and from a tool call.\n",
    "\n",
    "Let's create a list of messages. Each message can be supplied with a few things:\n",
    "\n",
    "* `content` - content of the message\n",
    "* `name` - optionally, who is creating the message\n",
    "* `response_metadata` - optionally, a dict of metadata that is often specific to each model provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a295879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Model\n",
      "\n",
      "So you said you were researching ocean mammals?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "Yes, that's right.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Model\n",
      "\n",
      "Great, what would you like to learn aboyut?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "I want to learn about the best places to see Orcas in the US.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from pprint import pprint\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "messages = [AIMessage(content=f'So you said you were researching ocean mammals?', name='Model')]\n",
    "messages.extend([HumanMessage(content=f\"Yes, that's right.\", name='Lance')])\n",
    "messages.extend([AIMessage(content=f'Great, what would you like to learn aboyut?', name='Model')])\n",
    "messages.extend([HumanMessage(content=f'I want to learn about the best places to see Orcas in the US.', name='Lance')])\n",
    "\n",
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2ad6b6",
   "metadata": {},
   "source": [
    "##### **Chat Models**\n",
    "\n",
    "Chat models can use a sequence of message as input and support message roles.\n",
    "\n",
    "There are many to choose from!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a80b1b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')\n",
    "result = llm.invoke(messages)\n",
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bbb5bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Orcas, also known as killer whales, can be seen in several locations across the U.S. Here are some of the best places to spot them:\n",
      "\n",
      "1. **Seattle and the San Juan Islands, Washington**: This region is one of the best places in the world to see orcas. The San Juan Islands are home to resident pods of orcas, particularly the Southern Resident killer whales. Various charter services offer whale-watching tours in the area.\n",
      "\n",
      "2. **Vancouver Island, British Columbia**: While it's technically in Canada, it's very accessible from the U.S., especially from Washington State. The waters around Victoria, British Columbia, are renowned for orca sightings.\n",
      "\n",
      "3. **California Coast**: While not as consistent as Washington, orcas can sometimes be seen offshore along the California coast, particularly around Monterey Bay. In winter and spring, transient orcas following their prey can be spotted.\n",
      "\n",
      "4. **Alaska**: Orcas can be seen in various locations in Alaska, including Glacier Bay National Park, Kenai Fjords National Park, and around the Aleutian Islands. There are numerous tour operators that provide opportunities to see orcas in these beautiful landscapes.\n",
      "\n",
      "5. **Florida**: While not a typical location for orcas, there have been occasional sightings in the Atlantic waters off the coast of Florida. However, these are rare and less predictable compared to other locations.\n",
      "\n",
      "For the best chances of sighting orcas, it's best to visit during the summer months when they are more active and have more predictable movements. Remember to check with local tour operators for the best times and locations for whale-watching.\n"
     ]
    }
   ],
   "source": [
    "result.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "114f5ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 330,\n",
       "  'prompt_tokens': 69,\n",
       "  'total_tokens': 399,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-4o-mini-2024-07-18',\n",
       " 'system_fingerprint': 'fp_64e0ac9789',\n",
       " 'id': 'chatcmpl-BLFMS1bZkbzap1gNnApWN45AEYowW',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b933e7c2",
   "metadata": {},
   "source": [
    "##### **Tools**\n",
    "\n",
    "Tools are need whenever you want a model to control parts of your code or call out to external APIs.\n",
    "\n",
    "Many LLMs providers supports tool calling.\n",
    "\n",
    "The tool calling interface in LangChain is simple.\n",
    "\n",
    "You can pass any Python function into `ChatModel.bind_tools()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0ca0c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a:int, b:int) -> int:\n",
    "    return a * b\n",
    "\n",
    "llm_with_tools = llm.bind_tools([multiply])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e7a4d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_XLx3I3P51x7jSxXMSC1jGGfK', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 51, 'total_tokens': 69, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_44added55e', 'id': 'chatcmpl-BLFUzITBmdoovSRRyUO0VCIPI3NZM', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-2eec0f5d-fe36-41ee-8340-91d15ec5e838-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_XLx3I3P51x7jSxXMSC1jGGfK', 'type': 'tool_call'}], usage_metadata={'input_tokens': 51, 'output_tokens': 18, 'total_tokens': 69, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call = llm_with_tools.invoke([HumanMessage(content=\"What is 2 multipled by 3\", name=\"Lucas\")])\n",
    "tool_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "106196f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'call_XLx3I3P51x7jSxXMSC1jGGfK',\n",
       "  'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'},\n",
       "  'type': 'function'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call.additional_kwargs['tool_calls']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd4f21c",
   "metadata": {},
   "source": [
    "##### **Using messages as state**\n",
    "\n",
    "With these foundations in place, we can now use `messages` in our graph state.\n",
    "\n",
    "Let's define our state `MessagesState`.\n",
    "\n",
    "It's defined as a `TypedDict` with a single key: `messages`.\n",
    "\n",
    "`messages` is simply a list of type `AntMessage`, meaning it's a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "638e9db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: list[AnyMessage]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f09977c",
   "metadata": {},
   "source": [
    "##### **Reducers**\n",
    "\n",
    "Now, we have a minor problem!\n",
    "\n",
    "As graph runs, we want to append messages to our `messages` state key.\n",
    "\n",
    "But each node will also override the prior state value.\n",
    "\n",
    "**Reducer functions** address this.\n",
    "\n",
    "They allow us to specify how state updates are performed.\n",
    "\n",
    "If no reducer function is explcitly specifeid, then it is assumed that all updates to that key should *override* it.\n",
    "\n",
    "Since we want to apped messages, we can user a pre-built `add_messages` reducer!\n",
    "\n",
    "This ensures that state updates you send to the graph are appended to the existing list of messages.\n",
    "\n",
    "We annotate (via `Annotated`) our key with a reducer function as metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28ea0946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff1693d",
   "metadata": {},
   "source": [
    "Since habing a list of messages in your state is so common, LangGraph has a pre-built `MessagesState`!\n",
    "\n",
    "`MessagesState` is defined:\n",
    "\n",
    "* With a pre-build single `messages` key\n",
    "* Which is a list of `AnyMessage` objects and uses the `add_messages` reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd2ccc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class State(MessagesState):\n",
    "    # add any keys needed beyond messages, which is pre-built\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56c6a22",
   "metadata": {},
   "source": [
    "The `MessagesState` and `State` both work equivalently!\n",
    "\n",
    "To go a bit deeper, we can see how the `add_messages` reducer works in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1a7d982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Hello! How can I assist you?', additional_kwargs={}, response_metadata={}, name='Model', id='42cdb7e5-c850-4713-b871-08ec3d221b02'),\n",
       " HumanMessage(content=\"I'm looking for information on marine biology.\", additional_kwargs={}, response_metadata={}, name='Lucas', id='07e195f4-b89c-4e80-accf-30e7356f7106'),\n",
       " AIMessage(content='Sure, I can help with that. What specifically are you interested in?', additional_kwargs={}, response_metadata={}, name='Model', id='e6b98bc3-ef6b-44bd-96db-55ab550b9a24')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial state\n",
    "initial_messages = [AIMessage(content='Hello! How can I assist you?', name='Model'),\n",
    "                    HumanMessage(content=\"I'm looking for information on marine biology.\", name='Lucas')]\n",
    "\n",
    "# new messsage to add\n",
    "new_message = AIMessage(content='Sure, I can help with that. What specifically are you interested in?', name='Model')\n",
    "\n",
    "# test\n",
    "add_messages(initial_messages, new_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7efc456",
   "metadata": {},
   "source": [
    "##### **Our graph**\n",
    "\n",
    "Now, let's use `MessagesState` with a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2587b5db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
